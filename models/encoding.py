import torch
import torch.nn.functional as F
from torch import nn


class LinearNumericalEncoding(nn.Module):
    def __init__(self, in_features: int, d_token: int, bias=True):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_features, d_token))
        self.bias = nn.Parameter(torch.randn(in_features, d_token)) if bias else None

    def forward(self, x_num: torch.Tensor) -> torch.Tensor:
        x_num = torch.einsum('bi,ij->bij', x_num, self.weight)

        if self.bias is not None:
            x_num += self.bias

        return x_num


class PiecewiseLinearEncoding(nn.Module):
    def __init__(self, boundaries: torch.Tensor, d_token: int):
        """
        :param boundaries: [d_numerical x n_boundaries] the boundaries generated by utils.data.generate_plc_boundaries
        :param d_token: the embedding dimension
        """
        super().__init__()
        self.d_numerical, n_boundaries = boundaries.size()
        n_bins = n_boundaries - 1

        self.register_buffer('boundaries', boundaries)
        self.register_buffer('boundary_range', torch.arange(0, n_bins))

        self.weight = nn.Parameter(torch.randn(self.d_numerical, n_bins, d_token))
        self.bias = nn.Parameter(torch.randn(self.d_numerical, d_token))

    def forward(self, x: torch.Tensor):
        """
        :param x: [batch_size x d_numerical]
        :return: [batch_size x d_numerical x (n_boundaries - 1)]
        """
        boundaries = self.get_buffer('boundaries')
        boundary_range = self.get_buffer('boundary_range')

        batch_size, d_numerical = x.size()
        d_numerical_b, n_boundaries = boundaries.size()
        n_bins = n_boundaries - 1

        assert d_numerical == d_numerical_b, 'Incompatible number of numerical features'

        result = torch.zeros(batch_size, d_numerical, n_boundaries - 1, device=x.device)

        for j, feature_boundaries in enumerate(boundaries):
            # handle cases where x < b_0 or x > b_T using min and max
            batch_feature_values = x[:, j]
            batch_feature_values = torch.maximum(feature_boundaries[0],
                                                 torch.minimum(feature_boundaries[-1], batch_feature_values))
            bin_indices = torch.minimum(torch.tensor(n_bins - 1, device=x.device),
                                        torch.bucketize(batch_feature_values, feature_boundaries, right=True))

            for batch, i in enumerate(bin_indices):
                result[batch][j][boundary_range < i] = 1
                selected_bin_size = feature_boundaries[i] - feature_boundaries[i - 1]

                # boundaries are not necessarily unique, this would result in some values being nan
                if selected_bin_size != 0:
                    result[batch][j][i] = (x[batch, j] - feature_boundaries[i - 1]) / (
                            feature_boundaries[i] - feature_boundaries[i - 1])

        result = torch.einsum('bij,ijk->bik', result, self.weight)
        result += self.bias
        result = F.relu(result)

        return result


class NLinear(nn.Module):
    """Linear layer with feature specific weights."""

    def __init__(self, in_features: int, d_in: int, d_out: int, bias=True):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_features, d_in, d_out))
        self.bias = nn.Parameter(torch.randn(in_features, d_out)) if bias else None

    def forward(self, x: torch.Tensor):
        """
        :param x: [batch_size x in_features x d_in]
        :return: [batch_size x in_features x d_out]
        """
        x = torch.einsum('bij,ijk->bik', x, self.weight)

        if self.bias is not None:
            x += self.bias

        return x


class PeriodicEncoding(nn.Module):
    def __init__(self, in_features: int, d_token: int, sigma=1.):
        super().__init__()
        assert d_token % 2 == 0, 'Embedding size must be divisible by 2'

        self.c = nn.Parameter(torch.empty(1, in_features, d_token // 2))
        self.sigma = sigma

        self.output = nn.Sequential(
            NLinear(in_features=in_features, d_in=d_token, d_out=d_token),
            nn.ReLU(),
        )

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.normal_(self.c, mean=0, std=self.sigma)

    def forward(self, x: torch.Tensor):
        """
        :param x: [batch_size x in_features] tensor containing numerical features
        :return: [batch_size x d_token]
        """
        x = 2 * torch.pi * (x.unsqueeze(dim=-1) * self.c)
        x = torch.cat((torch.sin(x), torch.cos(x)), dim=-1)
        x = self.output(x)
        return x
